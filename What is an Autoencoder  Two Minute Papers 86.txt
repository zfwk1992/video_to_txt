view fellow scholars this is too many papers with carriage on iPad as we have seen in earlier episodes of the series neural networks are remarkably efficient tools to solve a number of really difficult problems the first applications of neural networks usually revolve around classification problems classification means that we have an image as an input and the output is let's a simple decision whether it depicts a cat or a dog the input will have as many notes as there are pixels in the input image and the output will have to unit and will look at the one of these two that fires the most to decide whether it thinks it is a dog or a cat between these two there are hidden layers were the neural network is asked to build an inner representation of the problem that is efficient at recognising these anymore so what is an autoencoder an autoencoder is an interesting variant with two important changes first number of neurones is the same in the input and the output therefore we can expect that the output is an image that is not only the same size as the input but actually is the same image now this normally wouldn't make any sense why would he want to invent a neural network to do the job of a copying machine so here goes the second part we have a bottleneck in one of these later this means that the number of neutrons in that layer is much less than we would normally so therefore it has to find a way to represent this kind of data the best it can with a smaller number of neurons if you have a smaller budget you have to let go all the fluff and concentrate on the Bare Essentials therefore we can't expect the image to be the same but they are hopefully quite close this out of and colours are capable of creating sparse representations of the input data and can therefore be used for image compression I can't really avoid saying they are used awful for image compression autoencoders offer no tangible advantage over classical image compression algorithms like Jake however is a crumb of comfort many different variants exist that are useful for different tasks other than that there are denoising autoencoders that after learning the sparse representations can be presented with noisy image as they more or less know how this kind of data should look like they can help in the noise in these images that's pretty cool for starters what is even better is a variant that is called the variational autoencoder that not only learnt the sparse representations but can also draw new images as well we can for instance ask it to create new handwritten digits and we can actually expect the results to make sense there is an excellent blog post from France the creator of the amazing Charis library for building and training neural networks to have a look we were really only scratching the surface and I expect quite a few exciting autoencoder applications to pop up in the near future I cannot wait to get my pass on those papers hopefully you fellow scholars are also excited if you're interested in programming especially in Python make sure to check out the channel of centex for Thomas of machine learning programming videos and thanks for watching and for your generous support and I'll see you next time